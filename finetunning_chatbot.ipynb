{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a98b2fa-cbf5-40e0-bea8-feb7aa9015b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.45.2)\n",
      "Requirement already satisfied: datasets in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (3.0.1)\n",
      "Requirement already satisfied: torch in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.2.1+cu121)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: xxhash in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.68)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.11)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: datasets in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (3.0.1)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.15.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.32.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.25.2)\n",
      "Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.11)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets torch\n",
    "!pip install -q accelerate -U\n",
    "!pip install datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ca51e3",
   "metadata": {},
   "source": [
    "#### Finetunig de GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9662855-4aae-4fa3-b668-5d30c920e3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "489c07db-963e-49ca-b777-dd4dc9162830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble d'entraînement :\n",
      "                                                text\n",
      "0  Question: What should you do after rotating th...\n",
      "1  Question: What should be done if corrosion is ...\n",
      "2  Question: What should be done when re-connecti...\n",
      "3  Question: What type of screw is item 11 in the...\n",
      "4  Question: Can the cigar lighter socket be used...\n",
      "Ensemble de test :\n",
      "                                                text\n",
      "0  Question: What should be done to the rear axle...\n",
      "1  Question: What is the displacement of the Deut...\n",
      "2  Question: What should be done annually or ever...\n",
      "3  Question: What safety measures should be taken...\n",
      "4  Question: How do you shift gears? Réponse: Shi...\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Chemin vers votre dataset local\n",
    "file_path = 'fine-tuning data.csv'\n",
    "\n",
    "# Charger le dataset\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Combiner la question et la réponse pour en faire une seule colonne\n",
    "df['text'] = \"Question: \" + df['question'] + \" Réponse: \" + df['answer']\n",
    "\n",
    "# Supprimer les colonnes inutiles\n",
    "df = df[['text']]\n",
    "\n",
    "# Créer un Dataset Huggingface à partir du dataframe\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Diviser le dataset en ensembles d'entraînement et de test (80% train, 20% test)\n",
    "train_test_split = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# Accéder aux ensembles d'entraînement et de test\n",
    "data_train = train_test_split['train']\n",
    "data_test = train_test_split['test']\n",
    "\n",
    "# Créer un DatasetDict pour stocker les ensembles\n",
    "dataset_dict = DatasetDict({'train': data_train, 'test': data_test})\n",
    "\n",
    "# Convertir les ensembles en DataFrames Pandas pour visualiser les données\n",
    "train_df = data_train.to_pandas()\n",
    "test_df = data_test.to_pandas()\n",
    "\n",
    "# Visualiser les premières lignes de l'ensemble d'entraînement\n",
    "print(\"Ensemble d'entraînement :\")\n",
    "print(train_df.head())\n",
    "\n",
    "# Visualiser les premières lignes de l'ensemble de test\n",
    "print(\"Ensemble de test :\")\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38c6a750-65ba-4caa-9a8f-389a52095256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Écrire les données d'entraînement dans 'Q_A_train.txt'\n",
    "with open('Q_A_train.txt', 'w') as file:\n",
    "    for _, row in train_df.iterrows():\n",
    "        text = row['text']\n",
    "        file.write(f\"{text}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5608cf6-96ed-444d-afab-85388edaa997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the test data to 'Q_A_test.txt'\n",
    "with open('Q_A_test.txt', 'w') as file:\n",
    "    for row in data_test:\n",
    "        text = row['text']\n",
    "        file.write(f\"{text}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b14507f2-05b4-47b9-acd4-8e6bb3ecd0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "724c3673-b024-45fa-9fbe-324bb31351dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path, tokenizer, block_size = 128):\n",
    "    dataset = TextDataset(\n",
    "        tokenizer = tokenizer,\n",
    "        file_path = file_path,\n",
    "        block_size = block_size,\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abef44d4-3fa4-4d41-ae08-848753583aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_collator(tokenizer, mlm = False):\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=mlm,\n",
    "    )\n",
    "    return data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32819f41-4754-4d33-8532-ab77401381de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "def train(train_file_path,model_name,\n",
    "          output_dir,\n",
    "          overwrite_output_dir,\n",
    "          per_device_train_batch_size,\n",
    "          num_train_epochs,\n",
    "          save_steps):\n",
    "  tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "  train_dataset = load_dataset(train_file_path, tokenizer)\n",
    "  data_collator = load_data_collator(tokenizer)\n",
    "\n",
    "  tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "  model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "  model.save_pretrained(output_dir)\n",
    "\n",
    "  training_args = TrainingArguments(\n",
    "          output_dir=output_dir,\n",
    "          overwrite_output_dir=overwrite_output_dir,\n",
    "          per_device_train_batch_size=per_device_train_batch_size,\n",
    "          num_train_epochs=num_train_epochs,\n",
    "      )\n",
    "\n",
    "  trainer = Trainer(\n",
    "          model=model,\n",
    "          args=training_args,\n",
    "          data_collator=data_collator,\n",
    "          train_dataset=train_dataset,\n",
    "  )\n",
    "\n",
    "  trainer.train()\n",
    "  trainer.save_model()\n",
    "  #dakshi kamel kayn fl folder : chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfe643b8-aa0e-4b27-b5d5-3312a29decea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = \"Q_A_train.txt\"\n",
    "model_name = 'gpt2'\n",
    "overwrite_output_dir = False\n",
    "per_device_train_batch_size = 8\n",
    "num_train_epochs = 60.0\n",
    "save_steps = 50000\n",
    "output_dir = 'Chat_Model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd59dbdf-5b21-47b4-962e-33c4f5553b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14160' max='14160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14160/14160 1:08:15, Epoch 60/60]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.531400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.285800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.097800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.944700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.822900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.726300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.645000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.580100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.527500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.486700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.451300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.421900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.397000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.375700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.356100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.340100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.323200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.308100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.297000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.285600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.274800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.266400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.258800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.250200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.244000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.239500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.237300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(\n",
    "    train_file_path=train_file_path,\n",
    "    model_name=model_name,\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=overwrite_output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    save_steps=save_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "487fca60-26fe-4bf7-9dbe-6a53c01507c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel, GPT2TokenizerFast, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35fcf568",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Death or serious injury could result.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Fonction pour charger le modèle\n",
    "def load_model(model_path):\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "    model.to(\"cuda\")  # Assurez-vous que le modèle est sur le GPU si disponible\n",
    "    return model\n",
    "\n",
    "# Fonction pour charger le tokenizer\n",
    "def load_tokenizer(tokenizer_path):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n",
    "    return tokenizer\n",
    "\n",
    "# Fonction pour générer du texte\n",
    "def generate_text(model_path, sequence, max_length):\n",
    "    # Charger le modèle et le tokenizer\n",
    "    model = load_model(model_path)\n",
    "    tokenizer = load_tokenizer(model_path)\n",
    "\n",
    "    # Tokenizer la séquence d'entrée\n",
    "    ids = tokenizer.encode(sequence, return_tensors='pt').to(\"cuda\")  # Assurez-vous que les tensors sont sur le GPU\n",
    "\n",
    "    # Générer le texte\n",
    "    final_outputs = model.generate(\n",
    "        ids,\n",
    "        do_sample=True,\n",
    "        max_length=max_length,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "    )\n",
    "\n",
    "    # Décoder les prédictions\n",
    "    generated_text = tokenizer.decode(final_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extraire uniquement la réponse\n",
    "    # Supposons que la réponse commence après le mot \"Réponse:\"\n",
    "    answer_start = generated_text.find(\"Réponse:\") + len(\"Réponse:\")\n",
    "    answer_text = generated_text[answer_start:].strip()\n",
    "\n",
    "    # Extraire la réponse jusqu'à la fin ou jusqu'à la prochaine question\n",
    "    next_question_index = answer_text.find(\"Question:\")\n",
    "    if next_question_index != -1:\n",
    "        answer_text = answer_text[:next_question_index].strip()\n",
    "\n",
    "    return answer_text\n",
    "\n",
    "# Définir la séquence de question\n",
    "sequence = \"What could happen if the machine is operated or maintained improperly?\"\n",
    "max_len = 50\n",
    "model_path = \"Chat_Model\"  # Chemin vers votre modèle fine-tuné\n",
    "\n",
    "# Générer le texte\n",
    "generated_text = generate_text(model_path, sequence, max_len)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a40cf6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To provide operation and maintenance information for the SW405K wheel loader.\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Question: What is the purpose of this manual?\"\n",
    "max_len = 50\n",
    "model_path = \"Chat_Model\"  \n",
    "generated_text = generate_text(model_path, sequence, max_len)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7e98aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470-552-SANY (7269)\n"
     ]
    }
   ],
   "source": [
    "sequence = \"What is the phone number of SANY ?\"\n",
    "max_len = 50\n",
    "model_path = \"Chat_Model\"  \n",
    "\n",
    "\n",
    "generated_text = generate_text(model_path, sequence, max_len)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57b1cdf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A SANY dealer should be contacted.\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Who should you send the completed form to?\"\n",
    "max_len = 50\n",
    "model_path = \"Chat_Model\"  # Chemin vers votre modèle fine-tuné\n",
    "\n",
    "# Générer le texte\n",
    "generated_text = generate_text(model_path, sequence, max_len)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5405964b",
   "metadata": {},
   "source": [
    "#### Helsinki from English to darija"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "781b844c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'ترجم: سلام'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create the pipeline and specify device=0 to use the first GPU\n",
    "pipe = pipeline(\"text2text-generation\", model=\"lachkarsalim/Helsinki-translation-English_Moroccan-Arabic\", device=0)\n",
    "\n",
    "# Now the model will run on GPU\n",
    "result = pipe(\"Translate: Hello\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7eb9b6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "سلام, كيداير اليوم?\n"
     ]
    }
   ],
   "source": [
    "# Test the model with some English text\n",
    "text = \"Hello, how are you today?\"\n",
    "\n",
    "# Generate translation from English to Darija (Moroccan Arabic)\n",
    "translation = pipe(text)\n",
    "\n",
    "# Print the translated text\n",
    "print(translation[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85a07086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "عيّيت\n"
     ]
    }
   ],
   "source": [
    "# Input Darija text\n",
    "darija_text = \"I am tired\"\n",
    "\n",
    "# Try to generate the English translation\n",
    "translation = pipe(darija_text)\n",
    "\n",
    "# Print the translated txt (if it works)\n",
    "print(translation[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e5f08f",
   "metadata": {},
   "source": [
    "### Merging of GPT2 and Helsinki from enlish to darija"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6ed5e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline\n",
    "\n",
    "# Load both models and tokenizers\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"Chat_Model\").to(\"cuda\")\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"Chat_Model\")\n",
    "translation_pipe = pipeline(\"text2text-generation\", model=\"lachkarsalim/Helsinki-translation-English_Moroccan-Arabic\", device=0)\n",
    "\n",
    "# Function to generate answers and translate them\n",
    "# Function to generate answers and translate them\n",
    "def handle_input(input_text):\n",
    "    # Step 1: Generate the answer using GPT-2\n",
    "    input_ids = gpt2_tokenizer.encode(input_text, return_tensors='pt').to(\"cuda\")\n",
    "    \n",
    "    # Generate the answer (note: `stop_sequence` removed)\n",
    "    output = gpt2_model.generate(\n",
    "        input_ids, \n",
    "        max_length=50, \n",
    "        pad_token_id=gpt2_tokenizer.eos_token_id, \n",
    "        num_return_sequences=1, \n",
    "        early_stopping=True  # Enable early stopping to reduce over-generation\n",
    "    )\n",
    "    generated_answer = gpt2_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract the answer after \"Réponse:\" without including unintended extra questions\n",
    "    answer_start = generated_answer.find(\"Réponse:\") + len(\"Réponse:\")\n",
    "    answer_text = generated_answer[answer_start:].split(\"Question:\")[0].strip()  # Ensure splitting at the next \"Question:\"\n",
    "\n",
    "    # Step 2: Translate the original question and the generated answer into Darija\n",
    "    translated_question = translation_pipe(input_text)[0]['generated_text']\n",
    "    translated_answer = translation_pipe(answer_text)[0]['generated_text']\n",
    "    \n",
    "    # Return both the original answer and the translated version\n",
    "    return {\n",
    "        \"answer\": answer_text,\n",
    "        \"translated_question\": translated_question,\n",
    "        \"translated_answer\": translated_answer\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69ff1895",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:649: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: To provide operation and maintenance information for the SW405K wheel loader.\n",
      "Translated Question: شنو لهدف من هاد لكتيّاب?\n",
      "Translated Answer: باش نوفرو المعلومات ديال التشغيل و الصيانة للوادر SW405K.\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "user_input = \"What is the purpose of this manual?\"\n",
    "result = handle_input(user_input)\n",
    "\n",
    "print(\"Answer:\", result[\"answer\"])\n",
    "print(\"Translated Question:\", result[\"translated_question\"])\n",
    "print(\"Translated Answer:\", result[\"translated_answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80c53641",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:649: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: SANY\n",
      "Translated Question: علامن خسّك توسّل الاستمارة كاملة?\n",
      "Translated Answer: ساني\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "user_input = \"Who should you send the completed form to?\"\n",
    "result = handle_input(user_input)\n",
    "\n",
    "print(\"Answer:\", result[\"answer\"])\n",
    "print(\"Translated Question:\", result[\"translated_question\"])\n",
    "print(\"Translated Answer:\", result[\"translated_answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "339c645f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 470-552-SANY (7269)\n",
      "Translated Question: شنو نمرة ديال ساني?\n",
      "Translated Answer: 470-552-ساني (7269)\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "user_input = \"What is the phone number of SANY ?\"\n",
    "result = handle_input(user_input)\n",
    "\n",
    "print(\"Answer:\", result[\"answer\"])\n",
    "print(\"Translated Question:\", result[\"translated_question\"])\n",
    "print(\"Translated Answer:\", result[\"translated_answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17a7d97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d16672c9",
   "metadata": {},
   "source": [
    "### Helsinki from Darija to english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cbf78a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"translation\", model=\"lachkarsalim/LatinDarija_English-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d71557a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "# Translate the word \"salam\" from Darija to English\n",
    "translation = pipe(\"salam\")\n",
    "\n",
    "# Output the result\n",
    "print(translation[0]['translation_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "459c0ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey how are you\n"
     ]
    }
   ],
   "source": [
    "# Translate the word \"salam\" from Darija to English\n",
    "translation = pipe(\"سلام كيداير لباس عليك\")\n",
    "\n",
    "# Output the result\n",
    "print(translation[0]['translation_text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "170603b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is the purpose of this book\n"
     ]
    }
   ],
   "source": [
    "# Translate the word \"salam\" from Darija to English\n",
    "translation = pipe(\"شنو لهدف من هاد لكتاب\" )\n",
    "\n",
    "# Output the result\n",
    "print(translation[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a84354",
   "metadata": {},
   "source": [
    "### Final Merging of all the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2a51562",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline\n",
    "\n",
    "# Load models and tokenizers\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"Chat_Model\").to(\"cuda\")\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"Chat_Model\")\n",
    "\n",
    "# Pipeline for Darija to English translation\n",
    "darija_to_english_pipe = pipeline(\"translation\", model=\"lachkarsalim/LatinDarija_English-v2\", device=0)\n",
    "\n",
    "# Pipeline for English to Darija translation\n",
    "english_to_darija_pipe = pipeline(\"text2text-generation\", model=\"lachkarsalim/Helsinki-translation-English_Moroccan-Arabic\", device=0)\n",
    "\n",
    "# Function to handle input in Darija\n",
    "def handle_input_darija(input_text):\n",
    "    # Step 1: Translate the input question from Darija to English\n",
    "    translated_question = darija_to_english_pipe(input_text)[0]['translation_text']\n",
    "    \n",
    "    # Step 2: Generate the answer using GPT-2\n",
    "    input_ids = gpt2_tokenizer.encode(f\"Question: {translated_question}\", return_tensors='pt').to(\"cuda\")\n",
    "    \n",
    "    output = gpt2_model.generate(\n",
    "        input_ids, \n",
    "        max_length=50, \n",
    "        pad_token_id=gpt2_tokenizer.eos_token_id, \n",
    "        num_return_sequences=1, \n",
    "        early_stopping=True\n",
    "    )\n",
    "    generated_answer = gpt2_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract the answer after \"Réponse:\" and clean extra questions\n",
    "    answer_start = generated_answer.find(\"Réponse:\") + len(\"Réponse:\")\n",
    "    answer_text = generated_answer[answer_start:].split(\"Question:\")[0].strip()\n",
    "\n",
    "    # Step 3: Translate the generated answer from English back to Darija\n",
    "    translated_answer = english_to_darija_pipe(answer_text)[0]['generated_text']\n",
    "    \n",
    "    # Return the original question and translated answer\n",
    "    return {\n",
    "        \"original_question_in_darija\": input_text,\n",
    "        \"translated_question_in_english\": translated_question,\n",
    "        \"generated_answer_in_english\": answer_text,\n",
    "        \"translated_answer_in_darija\": translated_answer\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71608b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Question (Darija): شنو الهدف من هاد لكتاب؟\n",
      "Translated Question (English): What's the goal of a book?\n",
      "Generated Answer (English): To provide an overview of the subject matter and set the foundation for new information.\n",
      "Translated Answer (Darija): باش تعطي لمحات على الموضوع وترسي الأساس لماعلومات الجديدة.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "darija_input = \"شنو الهدف من هاد لكتاب؟\"  # \"What is the purpose of this manual?\" in Darija\n",
    "result = handle_input_darija(darija_input)\n",
    "\n",
    "print(\"Original Question (Darija):\", result[\"original_question_in_darija\"])\n",
    "print(\"Translated Question (English):\", result[\"translated_question_in_english\"])\n",
    "print(\"Generated Answer (English):\", result[\"generated_answer_in_english\"])\n",
    "print(\"Translated Answer (Darija):\", result[\"translated_answer_in_darija\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27f6e720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Question (Darija): شنو رقم الهاتف ديال ساني؟\n",
      "Translated Question (English): What's the phone number for Sanne?\n",
      "Generated Answer (English): 470-552-SANY (7269)\n",
      "Translated Answer (Darija): 470-552-ساني (7269)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "darija_input = \"شنو رقم الهاتف ديال ساني؟\"  # \"What is the purpose of this manual?\" in Darija\n",
    "result = handle_input_darija(darija_input)\n",
    "\n",
    "print(\"Original Question (Darija):\", result[\"original_question_in_darija\"])\n",
    "print(\"Translated Question (English):\", result[\"translated_question_in_english\"])\n",
    "print(\"Generated Answer (English):\", result[\"generated_answer_in_english\"])\n",
    "print(\"Translated Answer (Darija):\", result[\"translated_answer_in_darija\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28bcabf",
   "metadata": {},
   "source": [
    "### USER INTERFACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fd5376",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15884c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3adfd715448248d4baa323e2db5740e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Darija Question:', placeholder='Enter your question in Darija')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c626cf69241d45dba8d26e1fc55e4265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Submit', icon='check', style=ButtonStyle(), tooltip='Submit your q…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba8cbf9784414d47802a1f2cc7e307b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline\n",
    "\n",
    "# Load models and tokenizers\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"Chat_Model\").to(\"cuda\")\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"Chat_Model\")\n",
    "\n",
    "# Pipeline for Darija to English translation\n",
    "darija_to_english_pipe = pipeline(\"translation\", model=\"lachkarsalim/LatinDarija_English-v2\", device=0)\n",
    "\n",
    "# Pipeline for English to Darija translation\n",
    "english_to_darija_pipe = pipeline(\"text2text-generation\", model=\"lachkarsalim/Helsinki-translation-English_Moroccan-Arabic\", device=0)\n",
    "\n",
    "# Function to handle input in Darija\n",
    "def handle_input_darija(input_text):\n",
    "    # Translate the input question from Darija to English\n",
    "    translated_question = darija_to_english_pipe(input_text)[0]['translation_text']\n",
    "\n",
    "    # Generate the answer using GPT-2\n",
    "    input_ids = gpt2_tokenizer.encode(f\"Question: {translated_question}\", return_tensors='pt').to(\"cuda\")\n",
    "    output = gpt2_model.generate(\n",
    "        input_ids,\n",
    "        max_length=50,\n",
    "        pad_token_id=gpt2_tokenizer.eos_token_id,\n",
    "        num_return_sequences=1,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    generated_answer = gpt2_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract the answer after \"Réponse:\"\n",
    "    answer_start = generated_answer.find(\"Réponse:\") + len(\"Réponse:\")\n",
    "    answer_text = generated_answer[answer_start:].split(\"Question:\")[0].strip()\n",
    "\n",
    "    # Translate the generated answer from English back to Darija\n",
    "    translated_answer = english_to_darija_pipe(answer_text)[0]['generated_text']\n",
    "\n",
    "    return translated_answer\n",
    "\n",
    "# Create a text input box for Darija question\n",
    "input_box = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter your question in Darija',\n",
    "    description='Darija Question:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Create an output display\n",
    "output_box = widgets.Output()\n",
    "\n",
    "# Function to process the input when the button is clicked\n",
    "def on_button_click(b):\n",
    "    with output_box:\n",
    "        output_box.clear_output()  # Clear the previous output\n",
    "        darija_question = input_box.value\n",
    "        if darija_question:\n",
    "            with output_box:\n",
    "                print(\"Processing... Please wait.\")\n",
    "            answer = handle_input_darija(darija_question)\n",
    "            with output_box:\n",
    "                output_box.clear_output()\n",
    "                print(f\"Answer in Darija: {answer}\")\n",
    "\n",
    "# Create a button to trigger the processing\n",
    "button = widgets.Button(\n",
    "    description='Submit',\n",
    "    disabled=False,\n",
    "    button_style='success',  # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Submit your question',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "# Attach the button click event to the processing function\n",
    "button.on_click(on_button_click)\n",
    "\n",
    "# Display the input box, button, and output\n",
    "display(input_box)\n",
    "display(button)\n",
    "display(output_box)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ec4450",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
